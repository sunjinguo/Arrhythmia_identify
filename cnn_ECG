# -*- coding: utf-8 -*-
"""
Created on Tue Jun  6 14:07:58 2017

@author: Administrator
"""

import numpy as np;
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers.convolutional import Conv1D,MaxPooling1D
from keras.utils import np_utils
from keras.layers.core import Flatten
import keras
from keras.utils import plot_model


batch_size = 32
filters =32
kernel_size = 5
epochs =2
data_train=np.loadtxt(r'C:\Users\Administrator\Desktop\data-sun\data_sum_train_randperm.txt')
data_test=np.loadtxt(r'C:\Users\Administrator\Desktop\data-sun\data_sum_test_randperm.txt')
train=np.array(data_train)
test=np.array(data_test)
#np.random.shuffle(train)
#np.random.shuffle(test)
print(train.shape)
x_train=train[:,0:300]
y_train=train[:,300]
x_test=test[:,0:300]
y_test=test[:,300]
print(x_train)
print(y_train)
print(x_train.shape)
print(x_test.shape)
x_train=np.expand_dims(x_train,axis=2)
x_test=np.expand_dims(x_test,axis=2)
print(x_train.shape)
print(x_test.shape)
#np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))
#np.reshape(x_test,(1,x_test.shape[0],x_test.shape[1],1))

#y_train=np.expand_dims(y_train,axis=2)
#y_test=np.expand_dims(y_test,axis=2)

x_train = x_train.astype('double')
x_test = x_test.astype('double')

print(y_train.shape)
y_train = np_utils.to_categorical(y_train, 5)
y_test = np_utils.to_categorical(y_test, 5)
print(y_train)
#y_train=np.expand_dims(y_train,axis=1)
#y_test=np.expand_dims(y_test,axis=1)
print(y_train.shape)
#np.reshape(x_train,(1,x_train.shape[0],x_train.shape[1]))
#np.reshape(x_test,(1,x_test.shape[0],x_test.shape[1]))

print('Build model...')
model = Sequential()

# we add a Convolution1D, which will learn filters
# word group filters of size filter_length:
model.add(Conv1D(32,
                 5,
                 padding='valid',
                 activation='relu',
                 strides=3,
                 input_shape=(300,1)))
# we use max pooling:
model.add(MaxPooling1D(pool_size=3,strides=2,padding='valid'))

model.add(Conv1D(64,
                 3,
                 padding='valid',
                 activation='relu',
                 strides=3))
# we use max pooling:
model.add(MaxPooling1D(pool_size=3,strides=2,padding='valid'))

# We add a vanilla hidden layer:
model.add(Flatten())
model.add(Dense(256))
model.add(Dropout(0.5))
model.add(Activation('relu'))

#model.add(Dense(128))
#model.add(Dropout(0.2))
#model.add(Activation('relu'))

#model.add(Dense(64))
#model.add(Dropout(0.2))
#model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(5))
model.add(Activation('softmax'))
#softmax
print(model.summary())

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
plot_model(model, to_file='model.png',show_shapes='true')
keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)
